---
title: "The Ames Iowa House Price Prediction"
author: 'Group Member: Yahui Zhou, Jiaxin Yang, Jiangyue Zhu, Jike Zhong, Tingwei Guan'
date: "2022-11-24"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(readr)
library(tidymodels)
library(psych)
library(stringr)
library(GGally)
library(DataExplorer)
library(xgboost)
library(data.table)
library(mltools)
library(glmnet)
library(ggplot2)
library(tidyr)
library(dplyr)
library(class)
library(pls)
```


## Part I: Exploratory Data Analysis
### Part I.A Check data - (1) Basic information about data
```{r}
train <- read.csv("train.csv", header=TRUE)
test <- read.csv("test_new.csv", header=TRUE)

print("Basic information for training dataset")
print(is.data.frame(train))
print(dim(train))

print("Basic information for training dataset")
print(is.data.frame(test))
print(dim(test))

str(train)
```


### Part I.A Check data - (2) Data type

After checking the data, we found that the data type in the dataset is not very accurate. Some categorical data is stored as numeric type. Thus, we manually listed out the categorical and numerical variables and correct the data type accordingly.\

We have two types of variables: 52 Categorical variables, and 28 Numeric variables.\

Categorical variables (52)\
-- Nominal (27)\
-- Ordinal (25)

Nominal (27): MSSubClass, MSZoning, Street, Alley, LotShape, LandContour, Utilities, LotConfig, Neighborhood, Condition1, Condition2, BldgType, HouseStyle, RoofStyle, CentralAir, RoofMatl, Exterior1st, Exterior2nd, MasVnrType, Foundation, Heating, Functional, GarageType, Fence, MiscFeature, SaleType, SaleCondition. 

Ordinal (25): id, LandSlope, OverallQual, OverallCond, YearBuilt, YearRemodAdd, ExterQual, ExterCond, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, HeatingQC, Electrical, KitchenQual, FireplaceQu, GarageYrBlt, GarageFinish, GarageQual, GarageCond, PoolQC, MoSold, YrSold, PavedDrive.

Numeric variables (28): LotFrontage, LotArea, MasVnrArea, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, 1stFlrSF, 2ndFlrSF, LowQualFinSF, GrLivArea, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, Bedroom, Kitchen, TotRmsAbvGrd, Fireplaces, GarageCars, GarageArea, WoodDeckSF, OpenPorchSF, EnclosedPorch, 3SsnPorch, ScreenPorch, PoolArea, MiscVal. 

```{r}
# check data type
# find categorical and numerical variables
category_col = c("Id", "LandSlope", "OverallQual", "OverallCond", "YearBuilt",
                 "YearRemodAdd", "ExterQual", "ExterCond", "BsmtQual", "BsmtCond",
                 "BsmtExposure", "BsmtFinType1", "BsmtFinType2", "HeatingQC",
                 "Electrical", "KitchenQual", "FireplaceQu", "GarageFinish",
                 "GarageQual", "GarageCond", "PoolQC", "MoSold", "YrSold", "PavedDrive",
                 "MSSubClass", "MSZoning", "Street", "Alley", "LotShape", "LandContour",
                 "Utilities", "LotConfig", "Neighborhood", "Condition1", "Condition2",
                 "BldgType", "HouseStyle", "RoofStyle", "CentralAir", "RoofMatl",
                 "Exterior1st", "Exterior2nd", "MasVnrType", "Foundation", "Heating",
                 "Functional", "GarageType", "Fence", "MiscFeature", "SaleType",
                 "SaleCondition")
numeric_col = c("LotFrontage", "LotArea", "MasVnrArea", "BsmtFinSF1", "BsmtFinSF2",
                "BsmtUnfSF", "TotalBsmtSF", "X1stFlrSF", "X2ndFlrSF", "LowQualFinSF",
                "GrLivArea", "BsmtFullBath", "BsmtHalfBath", "FullBath", "HalfBath",
                "BedroomAbvGr", "KitchenAbvGr", "TotRmsAbvGrd", "Fireplaces",
                "GarageCars", "GarageArea", "WoodDeckSF", "OpenPorchSF", "EnclosedPorch",
                "X3SsnPorch", "ScreenPorch", "PoolArea", "MiscVal", "GarageYrBlt") 
# change the type of variable
train = train %>% mutate_at(category_col, as.character)
train = train %>% mutate_at(numeric_col, as.integer)

# check whether the type is changed successfully
str(train)
```
Here, we can see the data type are correct.


### Part I.A Check data - (3) duplicate / null value
```{r}
# check number of duplicated records
sum(duplicated(train))

# check number/percentage of NA data
na_per = c()
col_names = c()
for (i in 2: 80) {
  if (sum(is.na(train[,i]))/dim(train)[1]*100 > 0) {
    na_per = append(na_per, sum(is.na(train[,i]))/dim(train)[1]*100)
    col_names = append(col_names, colnames(train)[i])
  }
}

# draw the visualization to see percentage of NA
df = as.data.frame(col_names, na_per)
pt = ggplot(data = df, aes(x = na_per, y = col_names)) +
      geom_bar(stat="identity") +
      labs(x = "Percentage of Null Data (%)", y = "")
pt
```

In the Data Processing section, we first checked the duplicated records and found there is no duplicated records.
Then, we checked the number and percentage of null values for each variable. We built a visualization for the variables with null values. After checking the data, we found that NA does not only stands for the missing data. For the categorical data, we found that NA means "Not Accessible". Thus, we replaced the "NA" in the categorical variables with "No". For the numeric variables, NA can mean 0 or missing. Thus, we decide to use the medium value to replace "NA".


```{r}
# replace the null value
train <- train %>% mutate_if(is.numeric, function(x) ifelse(is.na(x), median(x, na.rm=T),x))
train <- train %>% mutate_if(is.character, ~replace_na(., "No"))
test <- test %>% mutate_if(is.numeric, function(x) ifelse(is.na(x), median(x, na.rm=T),x))
test <- test %>% mutate_if(is.character, ~replace_na(., "No"))
print(dim(train))
print(dim(test))

head(train)

# check whether all the null values are solved
sum(is.na(train))
sum(is.na(test))

# We treated all the year as categorical value.
# Thus, here we change year back to categorical value after filling the null data
train$GarageYrBlt = as.character(train$GarageYrBlt)
test$GarageYrBlt = as.character(test$GarageYrBlt)
str(train)
```

### Part I.B EDA 

```{r}
dim(train)
```

There are 1460 observations and 81 variables in the training data set.
First, lets's start to explore the response - Sale Price (in dollars).

### Part I.B EDA - (1) Response: SalePrice

#### Part I.B EDA - (1.1) Histogram of SalePrice\

```{r}
ggplot(train, aes(x = SalePrice)) + 
  geom_histogram(bins = 50, col= "white") 
```

The plot is right-skewed, which means that there is less expensive house than inexpensive ones.\

#### Part I.B EDA - (1.2) Histogram of SalePrice
```{r}
summary(train$SalePrice)
```

The median of Sale Price of the houses is $163000.\
The mean of Sale Price of the houses is $180921.\
The least expensive house is $34900.\
The most expensive house is $755000.\

### Part I.B EDA - (2) Correlation
#### Part I.B EDA - (2.1) Correlation between numeric predictor variables \ 
Next, to investigate if there are early signs of variables are likely to be significant in predicting response. First, let's look at those numeric variables.
```{r}
num_data <- subset(train, select = c(LotFrontage, LotArea, MasVnrArea, BsmtFinSF1,
                                     BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, X1stFlrSF,
                                     X2ndFlrSF, LowQualFinSF, GrLivArea, BsmtFullBath,
                                     BsmtHalfBath, FullBath, HalfBath, BedroomAbvGr,
                                     KitchenAbvGr, TotRmsAbvGrd, Fireplaces, GarageCars,
                                     GarageArea, WoodDeckSF, OpenPorchSF, EnclosedPorch,
                                     X3SsnPorch, ScreenPorch, PoolArea, MiscVal))

# Correlation between numerical data 
cor_matrix = cor(num_data)

# We set 0.6 as the threshold for strong correlation
strong_cor = cor_matrix

# we fill all the absolute correlation value less than 0.6 as NA and the value on the diagonal as NA
strong_cor[abs(strong_cor) < 0.6] = NA
strong_cor[upper.tri(strong_cor, diag = TRUE)] = NA

# Then we find the variables with high correlation (>=0.6) 
index <- which(strong_cor >= 0.6 | strong_cor <= -0.6, arr.ind = T)
strong_cor_var = cbind.data.frame(var1 = rownames(strong_cor)[index[,1]], # get the row name 
                                  var2 = colnames(strong_cor)[index[,2]]) # get the column name 
strong_cor_var
```
From the correlation table above, we can clearly find that [BsmtFinSF1, BsmtFullBath], [TotalBsmtSF, X1stFlrSF], [GrLivArea, X2ndFlrSF], [HalfBath, X2ndFlrSF],[TotRmsAbvGrd, X2ndFlrSF],[GrLivArea, TotRmsAbvGrd], [GrLivArea, FullBath], [BedroomAbvGr, TotRmsAbvGrd], [GarageArea, GarageCars] have appear to be potentially problematic collinearity amongst the predictor variables.


#### Part I.B EDA - (2.2) Correlation between numeric predictor variables and response variable
```{r}
corr_xy = cor(train[,unlist(lapply(train, is.numeric))])
y_col = ncol(cor(train[,unlist(lapply(train, is.numeric))]))
corr_xy_df = cbind.data.frame(SalePrice=cor(train[,unlist(lapply(train, is.numeric))])[,y_col])
corr_xy_df
```

From the last part, we find that the variables $TotalBsmtSF$, $X1stFlrSF$, $GrLivArea$, $GarageCars$, $GarageArea$ have strong correlation with the response $SalePrice$.

```{r}
strong_collearity <- subset(train, 
                            select = c(TotalBsmtSF, X1stFlrSF, GrLivArea, SalePrice))
pairs.panels(strong_collearity)
```

From the plot, we can verify that all of these three variables \
(1) $TotalBsmtSF$ (Total square feet of basement area)\
(2) $X1stFlrSF$ (First Floor square feet)\
(3) $GrLivArea$ (Above grade (ground) living area square feet) \
have strong positive correlation with the response $SalePrice$. \
In other words, as each of these three factors increases, $SalePrice$ will get increase.\

In addition, we would also explore those categorical variables that might be useful for predicting the response. \

#### Part I.B EDA - (2.3) Correlation between categorical predictor variables and response variable
#### Part I.B EDA - (2.3) Categorical Predictor 1: OverallQual\

$OverallQual$:

```{r}
overallQual <- as.numeric(train$OverallQual)
cor(overallQual, train$SalePrice)
```

Thus, $OverallQual$ has a strong positive correlation with the $SalePrice$.\


```{r}
ggplot(train, aes(x=overallQual,y=SalePrice)) + 
  geom_point(shape=20) + 
  geom_smooth(method="lm", color = "red") + 
  scale_y_continuous(labels=comma)
```

This plot verifies that $OverallQual$ has a strong positive correlation with the $SalePrice$.\


```{r}
train$OverallQual=as.factor(train$OverallQual)
train%>%ggplot(aes(x=OverallQual,y=SalePrice))+geom_violin(aes(fill=OverallQual))
```

OverallQual: Rates the overall material and finish of the house.\

       10	Very Excellent
       9	Excellent
       8	Very Good
       7	Good
       6	Above Average
       5	Average
       4	Below Average
       3	Fair
       2	Poor
       1	Very Poor



From the plot above, we can find that as Rates the overall material and finish of the house increases, the sale price of the house gets increased. Besides, if the rates the overall material and finish of the house is below average, the sale price varies for the largest range other than that of other rates. In addition, there is no big difference of the mean sale price of house at rate = 6 and rate = 7.\

#### Part I.B EDA - (2.3) Categorical Predictor 2: Neighboorhood\

Also, we can find that $Neighboorhood$ is also a good predictor for its positive strong correlation with $SalePrice$.\

```{r}
cor(train$TotalBsmtSF , train$SalePrice)
```

```{r}
ggplot(train, aes(x=Neighborhood,y=SalePrice)) + 
  geom_boxplot(fill="light blue", color="black")+
  theme(axis.text.x=element_text(angle = 90)) + 
  scale_y_continuous(labels=comma)
# change the scale from e-x type into real number
```

The neighborhoods of the house plays a significant role in the sale price. We can see that the houses around MeadowV were sold at the least expensive price, while those besides StroneBr were sold at the most expensive price. For houses' neighborhood is NoRidge, there are some of the most expensive price.

#### Part I.B EDA - (2.3) Categorical Predictor 2: GarageCars\

Moreover, for the $GarageCars$, we would like to explore by plotting.\

```{r}
train$GarageCars=as.factor(train$GarageCars)
train%>%ggplot(aes(x=GarageCars,y=SalePrice))+geom_violin(aes(fill=GarageCars))
```

We can see that for the house of size of garage in car capacity as 3, the sale price is the highest, while the houses with size of garage in car capacity as 0 have the most inexpensive sale price.\

At last, we infer that the age of house might be an important predictor.\

#### Part I.B EDA - (2.3) Categorical Predictor 4: YearBuilt\

House's age since being built (2022-YearBuilt):\

```{r}
year <- as.numeric(train$YearBuilt)
AgeHouse <- 2022 - year
```

```{r}
cor(AgeHouse, train$SalePrice)
```

It shows that there is strong negative correlation between the age of house and the sale price.\

```{r}
ggplot(train, aes(x=AgeHouse,y=SalePrice)) + geom_point(shape=20) + geom_smooth(method="lm", color = "red")+ scale_y_continuous(labels=comma)
```

As the age of house increases, the sale price will get decreasing.\

In brief, based on the EDA, there are lots of variables having weak correlation with the response. Thus, we would like to choose the LASSO and ridge regression for modeling.\

## Part II: Model Analysis
### Part II.A - Motivation 
Housing market is an important sector of the economy. Having an accurate prediction of housing price is of interest to the general public and the economic forecast. Conventional models for predictions include regression, decision trees, naive bayes, recurrent neural networks, etc. A good model should be able to generalize well to the test data, meaning that it should aim to capture the global minimum (in a strong convex optimization problem) without over-fitting or under-fitting, this means we need to take into account the bias-variance trade-off. \

Motivated by these insights, \
(1) we propose using Lasso (L1 regularization) for this particular task. Fitting a lasso-based regression model should ensure enough model capacity while minimizing the chance of over-fitting through regularization. It also has advantage over neural network given our limited amount of data samples. To find the best modeling strategy, we also test Ridge and Linear Regerssion along with Lasso. \
(2) Though the sample size here is larger than the number of variables, which means there is no high-dimensional problem, we find there are collinearity problems among the predictor variables through EDA. Thus, we choose to use PLS and PCA based model - PCR to overcome this problems.

### Part II.B - Math
### (1) Linear Regression Model:
$$Y_i = \beta_o + \sum_{k=1}^p\beta_kX_k+e_i$$ 
$$e_i \stackrel{iid}\sim N(0, \sigma^2)$$
### (2) LASSO:
$$\min_{\beta _{0},\beta } \left\{\sum_{i=1}^{N}\left ( y_{i}-\beta _{0}-x_{i}^{T}\beta  \right )^{2}\right\} subject \; to \sum_{j=1}^{p}\left| \beta _{j}\right|\leq t$$
where against $s=t(\lambda)$ \


### (3) Ridge:
$$\min_{\beta _{0},\beta } \left\{\sum_{i=1}^{N}\left ( y_{i}-\beta _{0}-x_{i}^{T}\beta  \right )^{2}\right\} subject \; to \sum_{j=1}^{p} \beta _{j}^{2}\leq t$$
where against $s=t(\lambda)$\


### (4) PCR:\
Let $Z_1, Z_2, ..., Z_m$ represent M < p linear combinations of our original p predictors. That is
$$Z_{m} = \sum_{j=1}^{p}\phi_{jm} x_{j}$$
$$z_{im} = u_{mp}x_{ip}$$
for some constants $\phi_{1m}, \phi_{2m}, ..., \phi_{pm}, m = 1, ..., M$. We can then fit the linear regression model.
$$y_i = \theta_0+\sum_{m=1}^M\theta_mz_{im}+\epsilon_i$$\ i = 1, ..., n using least square.
Note that in the upper equation, the regression coefficients are given by $\theta_0, \theta_1, ..., \theta_M$\


### (5) PLS:\
Set $$U_{mp} = \hat{\alpha}_p$$ from the regression model 
$$y_i = \alpha_0+\alpha_pX_{ip}^{(m)}+\epsilon_i$$ and calculate $$z_{im} = \sum_{p=1}^PU_{mp}x_{ip}$$ for each p = 1, ...,P
$$y_i = \theta_0+\sum_{m=1}^M\theta_mz_{im}+\epsilon_i$$\ i = 1, ..., n using least square.\

### Part II.C - Prepocess before applying model
We find that the predictions in train and test data set are not the same. Thus, we choose to remove the mismatch to ensure the same distribution between the train and test data set.\
```{r}
x_train <-  model.matrix(SalePrice~ . -1 , data = train)
y_train <-  train$SalePrice
x_test <-  model.matrix(SalePrice~ . -1 , data = test)
y_test <-  test$SalePrice
print(dim(x_train))
print(dim(x_test))
#x_train-x_test
missing_cols = c()
for (var in colnames(x_train)){
  if (!(var %in% colnames(x_test))){
    missing_cols <- c(missing_cols, var)
  }
}

#x_test-x_train
missing_cols_2 = c()
for (var in colnames(x_test)){
  if (!(var %in% colnames(x_train))){
    missing_cols_2 <- c(missing_cols_2, var)
  }
}
#we simply remove the mismatch to ensure the same distribution between train and test dataset
x_train <- x_train[, !colnames(x_train) %in% missing_cols]
x_test <- x_test[, !colnames(x_test) %in% missing_cols_2]

train = train[, !colnames(train) %in% missing_cols]
train = train[, !colnames(train) %in% missing_cols_2]

test = test[, !colnames(test) %in% missing_cols]
test = train[, !colnames(train) %in% missing_cols]
```

### Part II.D - Assumptions
Since Ridge Regression and Lasso Regression are special cases of the General Linear Model. They add penalty terms but otherwise all of the same conditions apply.\
The normal linear regression model assumes: $$Y_i = \beta_o + \sum_{k=1}^p\beta_kX_k+e_i$$ 
$$e_i \stackrel{iid}\sim N(0, \sigma^2)$$

The final set of model assumptions for linear regression models are:\
(1) Mean Function: $E(e_i|X)=0$.\
(2) Variance Function: $Var(e_i|X)=\sigma^2$.\
(3) Normality of the errors \
(4) Independence of the errors.\
(5) Little/No Multicollinearity in data.\

(4.1) Check the Mean Function: $E(e_i|X)=0$.\
```{r}
model.lm = lm(SalePrice ~., data = train)
plot(fitted(model.lm), resid(model.lm), xlab = "fitted values", 
     ylab = "residuals", main = "Residual Plot for SalePrice")
abline(h = 0)
```
Based on the graph, it is noted that though there are few outliers, most dots are around 0, which means that the mean of the error is approximately 0, which meet the assumption(1) of the model. \
Thus, the fitted mean function is appropriate.\

(4.2) Check Variance Function: $Var(e_i|X)=\sigma^2$.\
```{r}
model.lm = lm(SalePrice ~., data = train)
plot(fitted(model.lm), rstandard(model.lm), xlab = "fitted values", 
     ylab = "standard residuals", main = "")
abline(h = 0)
```
Based on the graph, it is noted that though there are few outliers, most dots are around 0 (a constant), which means that the variance of the error is approximately constant, which meet the assumption(2) of the model. \
Thus, the fitted variance function is appropriate.\

(4.3) Check Normality of the errors\
Our two main graphical approaches will be: Histogram and Normal probability plot:\
```{r}
hist(resid(model.lm), xlab="residuals",main="")
```
Based on the histogram of residuals, it is noted that the graph is approximately symmetry and cut in half, each side is the mirror of the other. \
Thus, the residuals are normally distributed, which means that the assumption(3) in the model is correct.\
```{r}
qqnorm(resid(model.lm)); qqline(resid(model.lm))
```

Based on the plot above, though we can see that points on the lower-end have lower measurement than the Normal model predicts and the points on the upper-end have higher measurement than the Normal model predicts, most points are approximately on the line. It might due to the outliers. \
Thus, the residuals are normally distributed, which means that the assumption(3) in the model is correct.\

(4.4) Check independence of the errors.\
Common violation of independence in regression models are often related to structure in the mechanism that generated thye sample: error for data collected sequentially/spatially/clusters. The error for data collected spatially might cause the dependence of the error issue in this dataset, but it is hard to check with the available dataset. \
Thus, we assume there is no such problem.\

(4.5) Check the independence of the variables.\
Based on the EDA in Part I, we can see that there are multicollineary among the variables. \
Thus, it fails to meet the assumption(5). \

Overall:\
The final set of model assumptions are:\
(1) Mean Function: $E(e_i|X)=0$.\
(2) Variance Function: $Var(e_i|X)=\sigma^2$.\
(3) Normality of the errors \
(4) Independence of the errors.\
(5) Little/no Multicollinearity in data.\
After checking, we see that the assumptions 1-3 are met. It is hard to check whether assumption 4 is met. Here, we assume the independence of the error. \
Through the EDA in Part I, we see that there are collinearly issue in our dataset. In consideration of some useless variables exist in our data set, we still want to use Lasso to zero out them and see how many variables can help us to predict the price of the house.\

### Part II.E - Validation
Here we explain one important step to the data augmentation. For fairness purposes, we need to ensure that our train and test distributions are the same (e.g. predictors existing in test data must also exist in train data). For this reason, we first compute the confusion matrix of both the train and test data, we then drop the features that are in the disjoint set of train and test sets. \

### Part II.F - Models

### Part II.F - Models - (1) Linear Regression Model
```{r}
set.seed(4620)
model.lm = lm(SalePrice~.,data = as.data.frame(train))
pred <- predict(model.lm, newdata = as.data.frame(test))
mse_lm = mean((pred-y_test)^2)
mse_lm
```

### Part II.F - Models - (2) Lasso 
```{r}
set.seed(4620)
cv.lasso <-  cv.glmnet(x_train, y_train, type.measure = "mse", alpha = 1)
plot(cv.lasso)
model.lasso =glmnet(x_train,y_train,lambda=cv.lasso$lambda.min, alpha=1)
pred <- predict(model.lasso,x_test)
mse_lasso = mean((pred-y_test)^2)
mse_lasso
```

### Part II.F - Models - (3) Ridge
```{R}
set.seed(4620)
cv.ridge <-  cv.glmnet(x_train, y_train, type.measure = "mse", alpha = 0)
plot(cv.ridge)
model.ridge = glmnet(x_train,y_train,lambda=cv.ridge$lambda.min, alpha=0)
pred <- predict(model.ridge,x_test)
mse_ridge = mean((pred-y_test)^2)
mse_ridge
```

### Part II.F - Models - (4) PCR
```{R}
set.seed(4620)
model.pcr = pcr(SalePrice~.,data=train, validation="CV")
# summary(model.pcr)
validationplot(model.pcr,val.type="MSEP", ylim=c(0,9999999999))
model.pcr2 = pcr(SalePrice~.,data=train,scale=TRUE,ncomp=170)
pcr.pred=predict(model.pcr2,ncomp=170)
mse_pcr = mean((as.vector(pcr.pred)-y_test)^2)
mse_pcr
```
For the dataset, it looks like the smallest CV error occurs when we use 170 principal components in the regression for SalePrice This is fewer than the total number of predictors in the dataset (347), so it seems like the dimension-reduction in PCR gaining us much.

### Part II.F - Models - (5) PLS
```{R}
set.seed(4620)
model.plsr = plsr(SalePrice~.,data=train,validation="CV")
# summary(model.plsr)
validationplot(model.plsr,val.type="MSEP")

model.plsr2 = plsr(SalePrice~.,data=train,scale=TRUE,ncomp=23)
pls.pred=predict(model.plsr2,ncomp=23)
mse_pls = mean((as.vector(pls.pred)-y_test)^2)
mse_pls
```
For the dataset, it looks like the smallest CV error occurs when we use 23 principal components in the regression for SalePrice This is fewer than the total number of predictors in the dataset (347), so it seems like the dimension-reduction in PLS gaining us much.\

#### Part II.G - Comparison\
```{r}
models = c("LM","Lasso","Ridge","PCR","PLS")
mse = c(mse_lm, mse_lasso, mse_ridge, mse_pcr, mse_pls)
compare = data.frame(mse, models)
ggplot(compare, aes(x=models,y=mse)) +
    geom_bar(stat="identity")+labs(x= "Models", y="MSE") 

```


```{r}
models = c("Lasso","Ridge")
mse = c(mse_lasso, mse_ridge)
compare2 = data.frame(mse, models)
ggplot(compare2, aes(x=models,y=mse))+geom_bar(stat="identity")+labs(x= "Models", y="MSE") 

```

#### Part II.H - Results\
Based on the MSE of 4 models, we find that\
(1) Linear Regression Model has larger MSE than Lasso and Ridge Regression. \
(2) PLS and PCR have comparably similar MSE, while Lasso and Ridge Regression have comparably similar MSE.\
(3) Lasso and Ridge Regression's MSE much smaller than PLS and PCR's MSE.\
(4) Lasso has the lowest MSE value.\

To conclude, since Lasso and Ridge Regression's MSE are much smaller than Linear Regression, it means that there are many unnecessary variables in the dataset, which would have a negative influence on the prediction. Also, since Lasso and Ridge Regression's MSE much smaller than PLS and PCR's MSE, it means that the useless variables in the dataset have a larger negative influence on the models than the collinearity problems among the predictors. Thus, using Lasso regression model can help us to zero out the unnecessary factors and predict the SalePrice more accurately.

