---
title: "The Ames Iowa House Price Prediction"
author: 'Group_2: Yahui Zhou, Jiaxin Yang, Jiangyue Zhu, Jike Zhong, Tingwei Guan'
date: "2022-11-24"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(readr)
library(tidymodels)
library(psych)
library(stringr)
library(GGally)
library(DataExplorer)
library(xgboost)
library(data.table)
library(mltools)
library(glmnet)
library(ggplot2)
library(tidyr)
library(dplyr)
library(class)
library(pls)
```


## Part I: Exploratory Data Analysis
### Check data - (1) Basic information about data
```{r}
train <- read.csv("train.csv", header=TRUE)
test <- read.csv("test_new.csv", header=TRUE)

print(dim(train))
print(dim(test))
print("Basic information for training dataset")
print(is.data.frame(train))
print(ncol(train))
print(nrow(train))

print("Basic information for training dataset")
print(is.data.frame(test))
print(ncol(test))
print(nrow(test))

str(train)
```


### Check data - (2) Data type

After checking the data, we found that the data type in the dataset is not very accurate. Some categorical data is stored as numeric type. Thus, we manually listed out the categorical and numerical variables and correct the data type accordingly.\

We have two types of variables: 52 Categorical variables, and 28 Numeric variables.\

Categorical variables (52)\
-- Nominal (27)\
-- Ordinal (25)

Nominal (27): MSSubClass, MSZoning, Street, Alley, LotShape, LandContour, Utilities, LotConfig, Neighborhood, Condition1, Condition2, BldgType, HouseStyle, RoofStyle, CentralAir, RoofMatl, Exterior1st, Exterior2nd, MasVnrType, Foundation, Heating, Functional, GarageType, Fence, MiscFeature, SaleType, SaleCondition. 

Ordinal (25): id, LandSlope, OverallQual, OverallCond, YearBuilt, YearRemodAdd, ExterQual, ExterCond, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, HeatingQC, Electrical, KitchenQual, FireplaceQu, GarageYrBlt, GarageFinish, GarageQual, GarageCond, PoolQC, MoSold, YrSold, PavedDrive.

Numeric variables (28): LotFrontage, LotArea, MasVnrArea, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, 1stFlrSF, 2ndFlrSF, LowQualFinSF, GrLivArea, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, Bedroom, Kitchen, TotRmsAbvGrd, Fireplaces, GarageCars, GarageArea, WoodDeckSF, OpenPorchSF, EnclosedPorch, 3SsnPorch, ScreenPorch, PoolArea, MiscVal. 

```{r}
# check data type
# find categorical and numerical variables
category_col = c("Id", "LandSlope", "OverallQual", "OverallCond", "YearBuilt", "YearRemodAdd", "ExterQual", "ExterCond", "BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2", "HeatingQC", "Electrical", "KitchenQual", "FireplaceQu", "GarageFinish", "GarageQual", "GarageCond", "PoolQC", "MoSold", "YrSold", "PavedDrive", "MSSubClass", "MSZoning", "Street", "Alley", "LotShape", "LandContour", "Utilities", "LotConfig", "Neighborhood", "Condition1", "Condition2", "BldgType", "HouseStyle", "RoofStyle", "CentralAir", "RoofMatl", "Exterior1st", "Exterior2nd", "MasVnrType", "Foundation", "Heating", "Functional", "GarageType", "Fence", "MiscFeature", "SaleType", "SaleCondition")
numeric_col = c("LotFrontage", "LotArea", "MasVnrArea", "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "X1stFlrSF", "X2ndFlrSF", "LowQualFinSF", "GrLivArea", "BsmtFullBath", "BsmtHalfBath", "FullBath", "HalfBath", "BedroomAbvGr", "KitchenAbvGr", "TotRmsAbvGrd", "Fireplaces", "GarageCars", "GarageArea", "WoodDeckSF", "OpenPorchSF", "EnclosedPorch", "X3SsnPorch", "ScreenPorch", "PoolArea", "MiscVal", "GarageYrBlt") 
# change the type of variable
train = train %>% mutate_at(category_col, as.character)
train = train %>% mutate_at(numeric_col, as.integer)

# check whether the type is changed successfully
str(train)
```
Here, we can see the data type are correct.


### Check data - (3) duplicate / null value
```{r}
# check number of duplicated records
sum(duplicated(train))

# check number/percentage of NA data
na_per = c()
col_names = c()
for (i in 2: 80) {
  if (sum(is.na(train[,i]))/dim(train)[1]*100 > 0) {
    na_per = append(na_per, sum(is.na(train[,i]))/dim(train)[1]*100)
    col_names = append(col_names, colnames(train)[i])
  cat(sprintf("%s \n # of NA: %d, Percentage: %.2f%% \n", colnames(train)[i],   sum(is.na(train[,i])), sum(is.na(train[,i]))/dim(train)[1]*100))
  }
}

# draw the visualization to see percentage of NA
df = as.data.frame(col_names, na_per)
pt = ggplot(data = df, aes(x = na_per, y = col_names)) +
      geom_bar(stat="identity") +
      labs(x = "Percentage of Null Data (%)", y = "")
pt
```

In the Data Processing section, we first checked the duplicated records and found there is no duplicated records.
Then, we checked the number and percentage of null values for each variable. We built a visualization for the variables with null values. After checking the data, we found that NA does not only stands for the missing data. For the categorical data, we found that NA means "Not Accessible". Thus, we replaced the "NA" in the categorical variables with "No". For the numeric variables, NA can mean 0 or missing. Thus, we decide to use the medium value to replace "NA".


```{r}
# replace the null value
train <- train %>% mutate_if(is.numeric, function(x) ifelse(is.na(x), median(x, na.rm=T),x))
train <- train %>% mutate_if(is.character, ~replace_na(., "No"))
test <- test %>% mutate_if(is.numeric, function(x) ifelse(is.na(x), median(x, na.rm=T),x))
test <- test %>% mutate_if(is.character, ~replace_na(., "No"))
print(dim(train))
print(dim(test))

head(train)

# check whether all the null values are solved
sum(is.na(train))
sum(is.na(test))

# We treated all the year as categorical value, thus, here we change year back to categorical value after filling the null data
train$GarageYrBlt = as.character(train$GarageYrBlt)
test$GarageYrBlt = as.character(test$GarageYrBlt)
str(train)
```

### EDA - (1) corr
```{r}
num_data <- subset(train, select = c(LotFrontage, LotArea, MasVnrArea, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, X1stFlrSF, X2ndFlrSF, LowQualFinSF, GrLivArea, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, TotRmsAbvGrd, Fireplaces, GarageCars, GarageArea, WoodDeckSF, OpenPorchSF, EnclosedPorch, X3SsnPorch, ScreenPorch, PoolArea, MiscVal, SalePrice))

# Correlation between numerical data 
cor_matrix = cor(num_data)

# We set 0.6 as the threshold for strong correlation
strong_cor = cor_matrix

# we fill all the absolute correlation value as NA and the value on the diagnoal as NA
strong_cor[abs(strong_cor) < 0.6] = NA
strong_cor[upper.tri(strong_cor, diag = TRUE)] = NA
strong_cor

# Then we find the variables with high correlation (>=0.6) 
index <- which(strong_cor >= 0.6 | strong_cor <= -0.6, arr.ind = T)
strong_cor_var = cbind.data.frame(var1 = rownames(strong_cor)[index[,1]], # get the row name 
                 var2 = colnames(strong_cor)[index[,2]]) # get the column name
strong_cor_var
```
From the correlation graph above, we can clearly find that [BsmtFinSF1, BsmtFullBath], [TotalBsmtSF, X1stFlrSF], [GrLivArea, X2ndFlrSF], [GrLivArea, TotRmsAbvGrd] have appear to be potentially problematic collinearity amongst the predictor variables.


### EDA - (2) graph

```{r}
dim(train)
```

There are 1460 observations and 81 variables in the training data set.
First, lets's start to explore the response - Sale Price (in dollars).


```{r}
ggplot(train, aes(x = SalePrice)) + 
  geom_histogram(bins = 50, col= "white") 
```

The plot is right-skewed, which means that there is less expensive house than inexpensive ones.


```{r}
summary(train$SalePrice)
```

The median of Sale Price of the houses is $163000.
The mean of Sale Price of the houses is $180921.
The least expensive house is $34900.
The most expensive house is $755000.


Next, to investigate if there are early signs of variables are likely to be significant in predicting response. First, let's look at those numeric variables.

```{r}
corr_xy = cor(train[,unlist(lapply(train, is.numeric))])
y_col = ncol(cor(train[,unlist(lapply(train, is.numeric))]))
corr_xy_df = cbind.data.frame(SalePrice=cor(train[,unlist(lapply(train, is.numeric))])[,y_col])
corr_xy_df
```

From the last part, we find that the variables $TotalBsmtSF$, $X1stFlrSF$, $GrLivArea$, $GarageCars$, $GarageArea$ have strong correlation with the response $SalePrice$.

```{r}
strong_collearity <- subset(train, select = c(TotalBsmtSF, X1stFlrSF, GrLivArea, SalePrice))
pairs.panels(strong_collearity)
```

From the plot, we can verify that all of these three variables $TotalBsmtSF$ (Total square feet of basement area), $X1stFlrSF$ (First Floor square feet), $GrLivArea$ (Above grade (ground) living area square feet) have strong positive correlation with the response $SalePrice$. In other words, as each of these three factors (Total square feet of basement area / First Floor square feet / Above grade (ground) living area square feet) increasing, $SalePrice$ will get increase.

In addition, we would also explore those categorical variables that might be useful for predicting the response. 

$OverallQual$:

```{r}
overallQual <- as.numeric(train$OverallQual)
cor(overallQual, train$SalePrice)
```

Thus, $OverallQual$ has a strong positive correlation with the $SalePrice$.


```{r}
ggplot(train, aes(x=overallQual,y=SalePrice)) + geom_point(shape=20) + geom_smooth(method="lm", color = "red")+ scale_y_continuous(labels=comma)
```

This plot verifies that $OverallQual$ has a strong positive correlation with the $SalePrice$.


```{r}
train$OverallQual=as.factor(train$OverallQual)
train%>%ggplot(aes(x=OverallQual,y=SalePrice))+geom_violin(aes(fill=OverallQual))
```

OverallQual: Rates the overall material and finish of the house.

       10	Very Excellent
       9	Excellent
       8	Very Good
       7	Good
       6	Above Average
       5	Average
       4	Below Average
       3	Fair
       2	Poor
       1	Very Poor



From the colorful plot, we can find that as Rates the overall material and finish of the house increasing, the sale price of the house gets increased. Besides, if the rates the overall material and finish of the house is below average, the sale price varies for the largest range other than that of other rates. In addition, there is no big difference of the mean sale price of house at rate = 6 and rate = 7.


Also, we can find that $Neighboorhood$ is also a good predictor for its positive strong correlation with $SalePrice$.

```{r}
cor(train$TotalBsmtSF , train$SalePrice)
```

```{r}
ggplot(train, aes(x=Neighborhood,y=SalePrice)) + geom_boxplot(fill="light blue", color="black")+
theme(axis.text.x=element_text(angle = 90)) + scale_y_continuous(labels=comma)
                                            # change the scale from e-x type into real number
```

The neighborhoods of the house plays a significant role in the sale price. We can see that the houses around MeadowV were sold at the least expensive price, while those besides StroneBr were sold at the most expensive price. For houses' neighborhood is NoRidge, there are some of the most expensive price.


Moreover, for the $GarageCars$, we would like to explore by plotting.

```{r}
train$GarageCars=as.factor(train$GarageCars)
train%>%ggplot(aes(x=GarageCars,y=SalePrice))+geom_violin(aes(fill=GarageCars))
```

We can see that for the house of size of garage in car capacity as 3, the sale price is the highest, while the houses with size of garage in car capacity as 0 have the most inexpensive sale price.

At last, we infer that the age of house might be an important predictor.

House's age since being built (2022-YearBuilt):

```{r}
year <- as.numeric(train$YearBuilt)
AgeHouse <- 2022 - year
```

```{r}
cor(AgeHouse, train$SalePrice)
```

It shows that there is strong negative correlation between the age of house and the sale price.

```{r}
ggplot(train, aes(x=AgeHouse,y=SalePrice)) + geom_point(shape=20) + geom_smooth(method="lm", color = "red")+ scale_y_continuous(labels=comma)
```

As the age of house increases, the sale price will get decreasing.


In brief, based on the EDA, there are lots of variables having weak correlation with the response. Thus, we would like to choose the LASSO and ridge regression for modeling.


## Part II: Model Analysis
### (1) Motivation 
Housing market is an important sector of the economy. Having an accurate prediction of housing price is of interest to the general public and the economic forecast. Conventional models for predictions include regression, decision trees, naive bayes, recurrent neural networks, etc. A good model should be able to generalize well to the test data, meaning that it should aim to capture the global minimum (in a strong convex optimization problem) without over-fitting or under-fitting, this means we need to take into account the bias-variance trade-off. \

Motivated by these insights, \
(1) we propose using Lasso (L1 regularization) for this particular task. Fitting a lasso-based regression model should ensure enough model capacity while minimizing the chance of over-fitting through regularization. It also has advantage over neural network given our limited amount of data samples. To find the best modeling strategy, we also test Ridge along with Lasso. \
(2) Though the sample size here is larger than the number of variables, which means there is no high-dimensional problem, we find there are collinearity problems among the predictor variables through EDA. Thus, we choose to use PLS and PCA based model - PCR to overcome this problems.

### (2) Math
(2.1) LASSO:
$$\min_{\beta _{0},\beta } \left\{\sum_{i=1}^{N}\left ( y_{i}-\beta _{0}-x_{i}^{T}\beta  \right )^{2}\right\} subject \; to \sum_{j=1}^{p}\left| \beta _{j}\right|\leq t$$
where against $s=t(\lambda)$ \


(2.2) Ridge:
$$\min_{\beta _{0},\beta } \left\{\sum_{i=1}^{N}\left ( y_{i}-\beta _{0}-x_{i}^{T}\beta  \right )^{2}\right\} subject \; to \sum_{j=1}^{p} \beta _{j}^{2}\leq t$$
where against $s=t(\lambda)$\


(2.3) PCR:\
Let $Z_1, Z_2, ..., Z_m$ represent M < p linear combinations of our original p predictors. That is
$$Z_{m} = \sum_{j=1}^{p}\phi_{jm} x_{j}$$
$$z_{im} = u_{mp}x_{ip}$$
for some constants $\phi_{1m}, \phi_{2m}, ..., \phi_{pm}, m = 1, ..., M$. We can then fit the linear regression model.
$$y_i = \theta_0+\sum_{m=1}^M\theta_mz_{im}+\epsilon_i$$\ i = 1, ..., n using least square.
Note that in the upper equation, the regression coefficients are given by $\theta_0, \theta_1, ..., \theta_M$\


(2.4) PLS:\
Set $$U_{mp} = \hat{\alpha}_p$$ from the regression model 
$$y_i = \alpha_0+\alpha_pX_{ip}^{(m)}+\epsilon_i$$ and calculate $$z_{im} = \sum_{p=1}^PU_{mp}x_{ip}$$ for each p = 1, ...,P
$$y_i = \theta_0+\sum_{m=1}^M\theta_mz_{im}+\epsilon_i$$\ i = 1, ..., n using least square.\

### (3) Prepocess before applying model
We find that the predictions in train and test data set are not the same. Thus, we choose to remove the mismatch to ensure the same distribution between the train and test data set
```{r}
x_train <-  model.matrix(SalePrice~ . -1 , data = train)
y_train <-  train$SalePrice
x_test <-  model.matrix(SalePrice~ . -1 , data = test)
y_test <-  test$SalePrice
print(dim(x_train))
print(dim(x_test))
#x_train-x_test
missing_cols = c()
for (var in colnames(x_train)){
  if (!(var %in% colnames(x_test))){
    missing_cols <- c(missing_cols, var)
  }
}

#x_test-x_train
missing_cols_2 = c()
for (var in colnames(x_test)){
  if (!(var %in% colnames(x_train))){
    missing_cols_2 <- c(missing_cols_2, var)
  }
}
#we simply remove the mismatch to ensure the same distribution between train and test dataset
x_train <- x_train[, !colnames(x_train) %in% missing_cols]
x_test <- x_test[, !colnames(x_test) %in% missing_cols_2]

train = train[, !colnames(train) %in% missing_cols]
train = train[, !colnames(train) %in% missing_cols_2]

test = test[, !colnames(test) %in% missing_cols]
test = train[, !colnames(train) %in% missing_cols]
```

### (4) Assumption
Since Ridge Regression and Lasso Regression are special cases of the General Linear Model. They add penalty terms but otherwise all of the same conditions apply.
The normal linear regression model assumes: $$Y_i = \beta_o + \beta_1X_1+e_i$$ 
$$e_i \stackrel{iid}\sim N(0, \sigma^2)$$

The final set of model assumptions are:\
(1) Mean Function: $E(e_i|X)=0$.\
(2) Variance Function: $Var(e_i|X)=\sigma^2$.\
(3) Normality of the errors \
(4) Independence of the errors.\
(5) Little/no Multicollinearity in data.\

(4.1) Check the Mean Function: $E(e_i|X)=0$.\
```{r}
model.lm = lm(SalePrice ~., data = train)
plot(fitted(model.lm), resid(model.lm), xlab = "fitted values", 
     ylab = "residuals", main = "Residual Plot for SalePrice")
abline(h = 0)
```
Based on the graph, it is noted that though there are few outliers, most dots are around 0, which means that the mean of the error is approximately zero, which meet the assumption(1) of the model. Thus, the fitted mean function is appropriate.\

(4.2) Check Variance Function: $Var(e_i|X)=\sigma^2$.\
```{r}
model.lm = lm(SalePrice ~., data = train)
plot(fitted(model.lm), rstandard(model.lm), xlab = "fitted values", 
     ylab = "standard residuals", main = "")
abline(h = 0)
```
Based on the graph, it is noted that though there are few outliers, most dots are around 0 (a constant), which means that the variance of the error is approximately constant, which meet the assumption(2) of the model. Thus, the fitted variance function is appropriate.\

(4.3) Check Normality of the errors\
Our two main graphical approaches will be: Histograms and Normal probability plot:\
```{r}
hist(resid(model.lm), xlab="residuals",main="")
```
Based on the histogram of residuals, it is noted that the graph is approximately symmetry and cut in half, each side is the mirror of the other. Thus, the residuals are normally distributed, which means that the assumption(3) in the model is correct.\
```{r}
qqnorm(resid(model.lm)); qqline(resid(model.lm))
```

Based on the plot above, though we can see that points on the lower-end have lower measurement than the Normal model predicts and the points on the upper-end have higher measurement than the Normal model predicts, most points are approximately on the line. It might due to the outliers. Thus, the residuals are normally distributed, which means that the assumption(3) in the model is correct.\

(4.4) Check independence of the errors.\
Common violation of independence in regression models are often related to structure in the mechanism that generated thye sample: error for data collected sequentially/spatially/clusters. The error for data collected spatially might cause the dependence of the error issue in this dataset, but it is hard to check with the available dataset. Thus, we assume there is no such problem.\

(4.5) Check the independence of the variables.\
Based on the EDA in Part I, we can see that there are multicollineary among the variables. Thus, it fails to meet the assumption(5). \

Overall:\
The final set of model assumptions are:\
(1) Mean Function: $E(e_i|X)=0$.\
(2) Variance Function: $Var(e_i|X)=\sigma^2$.\
(3) Normality of the errors \
(4) Independence of the errors.\
(5) Little/no Multicollinearity in data.\
After checking, we see that the assumptions 1-3 are met. It is hard to check whether assumption 4 is met. Here, we assume the independence of the error. Through the EDA in Part I, we see that there are collinearly issue in our dataset. In consideration of some useless variables exist in our data set, we still want to use Lasso to zero out them and see how many variables can help us to predict the price of the house.\

### (5) Validation
Here we explain one important step to the data augmentation. For fairness purposes, we need to ensure that our train and test distributions are the same (e.g. predictors existing in test data must also exist in train data). For this reason, we first compute the confusion matrix of both the train and test data, we then drop the features that are in the disjoint set of train and test sets. 

### (6) Models

#### (6.1) lasso
```{R}
set.seed(4620)
cv.lasso <-  cv.glmnet(x_train, y_train, type.measure = "mse", alpha = 1)
plot(cv.lasso)
model.lasso =glmnet(x_train,y_train,lambda=cv.lasso$lambda.min, alpha=1)
pred <- predict(model.lasso,x_test)
mse_lasso = mean((pred-y_test)^2)
mse_lasso
```

#### (6.2) ridge
```{R}
set.seed(4620)
cv.ridge <-  cv.glmnet(x_train, y_train, type.measure = "mse", alpha = 0)
plot(cv.ridge)
model.ridge = glmnet(x_train,y_train,lambda=cv.ridge$lambda.min, alpha=0)
pred <- predict(model.ridge,x_test)
mse_ridge = mean((pred-y_test)^2)
mse_ridge
```

#### (6.3) pcr
```{R}
set.seed(4620)
model.pcr = pcr(SalePrice~.,data=train, validation="CV")
# summary(model.pcr)
validationplot(model.pcr,val.type="MSEP", ylim=c(0,9999999999))
model.pcr2 = pcr(SalePrice~.,data=train,scale=TRUE,ncomp=170)
pcr.pred=predict(model.pcr2,ncomp=170)
mse_pcr = mean((as.vector(pcr.pred)-y_test)^2)
mse_pcr
```
For the dataset, it looks like the smallest CV error occurs when we use 170 principal components in the regression for SalePrice This is fewer than the total number of predictors in the dataset (347), so it seems like the dimension-reduction in PCR gaining us much.

#### (6.4) plsr
```{R}
set.seed(4620)
model.plsr = plsr(SalePrice~.,data=train,validation="CV")
# summary(model.plsr)
validationplot(model.plsr,val.type="MSEP")

model.plsr2 = plsr(SalePrice~.,data=train,scale=TRUE,ncomp=23)
pls.pred=predict(model.plsr2,ncomp=23)
mse_pls = mean((as.vector(pls.pred)-y_test)^2)
mse_pls
```
For the dataset, it looks like the smallest CV error occurs when we use 23 principal components in the regression for SalePrice This is fewer than the total number of predictors in the dataset (347), so it seems like the dimension-reduction in PLS gaining us much.

### (7) Comparision
```{r}
models = c("Lasso","Ridge","PCR","PLS")
mse = c(mse_lasso, mse_ridge, mse_pcr, mse_pls)
compare = data.frame(mse, models)
ggplot(compare, aes(x=models,y=mse)) +
    geom_bar(stat="identity")+labs(x= "Models", y="MSE") 

```

```{r}
models = c("Lasso","Ridge")
mse = c(mse_lasso, mse_ridge)
compare2 = data.frame(mse, models)
ggplot(compare2, aes(x=models,y=mse)) +
    geom_bar(stat="identity") +
    labs(x= "Models", y="MSE") 

```

### (8) Results

From the summary of all methods' MSE, we find that the PLS regression model has the highest MSE, while the MSE for ridge regression and lasso regression are similar and both of them are lower than PLS' and PCR's MSE. There is not much difference among the test errors resulting from lasso and ridge approaches, but lasso regression performs a smaller MSE which results in a more accurate estimate or forecast of the actual values. In other words, the useless variables have a larger negative influence on the models. Thus, for the dataset, lasso regression has the best performance in the accuracy of estimation with the smallest MSE result.
