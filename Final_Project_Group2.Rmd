---
title: "STAT4620_Final_Project"
author: "Group_2: Yahui Zhou, Jiaxin Yang, Jiangyue Zhu, Jike Zhong, Tingwei Guan"
date: "2022-11-24"
output: html_document
---

```{r}
library(readr)
library(tidymodels)
library(psych)
library(stringr)
library(GGally)
library(DataExplorer)
library(xgboost)
library(data.table)
library(mltools)
library(glmnet)
library(ggplot2)
library(tidyr)
library(dplyr)
library(e1071)
library(caTools)
library(class)
library(caret)
```


## Part I: Exploratory Data Analysis
### Check data - (1) Basic information about data
```{r}
train <- read.csv("train.csv", header=TRUE)
test <- read.csv("test_new.csv", header=TRUE)

print(dim(train))
print(dim(test))
print("Basic information for training dataset")
print(is.data.frame(train))
print(ncol(train))
print(nrow(train))

print("Basic information for training dataset")
print(is.data.frame(test))
print(ncol(test))
print(nrow(test))

str(train)
```


### Check data - (2) Data type
We have two types of variables: 52 Categorical variables, and 28 Numeric variables.

Categorical variables (52)
-- Nominal (27)
-- Ordinal (25)

Nominal (27): MSSubClass, MSZoning, Street, Alley, LotShape, LandContour, Utilities, LotConfig, Neighborhood, Condition1, Condition2, BldgType, HouseStyle, RoofStyle, CentralAir, RoofMatl, Exterior1st, Exterior2nd, MasVnrType, Foundation, Heating, Functional, GarageType, Fence, MiscFeature, SaleType, SaleCondition. 

Ordinal (25): id, LandSlope, OverallQual, OverallCond, YearBuilt, YearRemodAdd, ExterQual, ExterCond, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, HeatingQC, Electrical, KitchenQual, FireplaceQu, GarageYrBlt, GarageFinish, GarageQual, GarageCond, PoolQC, MoSold, YrSold, PavedDrive.

Numeric variables (28): LotFrontage, LotArea, MasVnrArea, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, 1stFlrSF, 2ndFlrSF, LowQualFinSF, GrLivArea, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, Bedroom, Kitchen, TotRmsAbvGrd, Fireplaces, GarageCars, GarageArea, WoodDeckSF, OpenPorchSF, EnclosedPorch, 3SsnPorch, ScreenPorch, PoolArea, MiscVal. 

```{r}
# check data type
# find categorical and numerical variables
category_col = c("Id", "LandSlope", "OverallQual", "OverallCond", "YearBuilt", "YearRemodAdd", "ExterQual", "ExterCond", "BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2", "HeatingQC", "Electrical", "KitchenQual", "FireplaceQu", "GarageFinish", "GarageQual", "GarageCond", "PoolQC", "MoSold", "YrSold", "PavedDrive", "MSSubClass", "MSZoning", "Street", "Alley", "LotShape", "LandContour", "Utilities", "LotConfig", "Neighborhood", "Condition1", "Condition2", "BldgType", "HouseStyle", "RoofStyle", "CentralAir", "RoofMatl", "Exterior1st", "Exterior2nd", "MasVnrType", "Foundation", "Heating", "Functional", "GarageType", "Fence", "MiscFeature", "SaleType", "SaleCondition")
numeric_col = c("LotFrontage", "LotArea", "MasVnrArea", "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "X1stFlrSF", "X2ndFlrSF", "LowQualFinSF", "GrLivArea", "BsmtFullBath", "BsmtHalfBath", "FullBath", "HalfBath", "BedroomAbvGr", "KitchenAbvGr", "TotRmsAbvGrd", "Fireplaces", "GarageCars", "GarageArea", "WoodDeckSF", "OpenPorchSF", "EnclosedPorch", "X3SsnPorch", "ScreenPorch", "PoolArea", "MiscVal", "GarageYrBlt") 
# change the type of variable
train = train %>% mutate_at(category_col, as.character)
train = train %>% mutate_at(numeric_col, as.integer)

# check whether the type is changed successfully
str(train)
```

After checking the data, we found that the data type in the dataset is not very accurate. Some categorical data is stored as numeric type. Thus, we manually listed out the categorical and numerical variables and correct the data type accordingly.

### Check data - (3) duplicate / null value
```{r}
# check number of duplicated records
sum(duplicated(train))

# check number/percentage of NA data
na_per = c()
col_names = c()
for (i in 2: 80) {
  if (sum(is.na(train[,i]))/dim(train)[1]*100 > 0) {
    na_per = append(na_per, sum(is.na(train[,i]))/dim(train)[1]*100)
    col_names = append(col_names, colnames(train)[i])
  cat(sprintf("%s \n # of NA: %d, Percentage: %.2f%% \n", colnames(train)[i],   sum(is.na(train[,i])), sum(is.na(train[,i]))/dim(train)[1]*100))
  }
}

# draw the visualization to see percentage of NA
df = as.data.frame(col_names, na_per)
pt = ggplot(data = df, aes(x = na_per, y = col_names)) +
      geom_bar(stat="identity") +
      labs(x = "Percentage of Null Data (%)", y = "")
pt
```

In the Data Processing section, we first checked the duplicated records and found there is no duplicated records.
Then, we checked the number and percentage of null values for each variable. We built a visualization for the variables with null values. After checking the data, we found that NA does not only stands for the missing data. For the categorical data, we found that NA means "Not Accessible". Thus, we replaced the "NA" in the categorical variables with "No". For the numeric variables, NA can mean 0 or missing. Thus, we decide to use the medium value to replace "NA".


```{r}
# replace the null value
train <- train %>% mutate_if(is.numeric, function(x) ifelse(is.na(x), median(x, na.rm=T),x))
train <- train %>% mutate_if(is.character, ~replace_na(., "No"))
test <- test %>% mutate_if(is.numeric, function(x) ifelse(is.na(x), median(x, na.rm=T),x))
test <- test %>% mutate_if(is.character, ~replace_na(., "No"))
print(dim(train))
print(dim(test))

head(train)

# check whether all the null values are solved
sum(is.na(train))
sum(is.na(test))

# We treated all the year as categorical value, thus, here we change year back to categorical value after filling the null data
train$GarageYrBlt = as.character(train$GarageYrBlt)
test$GarageYrBlt = as.character(test$GarageYrBlt)
str(train)
```

### EDA - (1) corr
```{r}
num_data <- subset(train, select = c(LotFrontage, LotArea, MasVnrArea, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, X1stFlrSF, X2ndFlrSF, LowQualFinSF, GrLivArea, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, TotRmsAbvGrd, Fireplaces, GarageCars, GarageArea, WoodDeckSF, OpenPorchSF, EnclosedPorch, X3SsnPorch, ScreenPorch, PoolArea, MiscVal, SalePrice))

# Correlation between numerical data 
cor_matrix = cor(num_data)
cor_matrix

# We set 0.6 as the threshold for strong correlation
strong_cor = cor_matrix

# we fill all the absolute correlation value as NA and the value on the diagnoal as NA
strong_cor[abs(strong_cor) < 0.6] = NA
strong_cor[upper.tri(strong_cor, diag = TRUE)] = NA
strong_cor

# Then we find the variables with high correlation (>=0.6) 
index <- which(strong_cor >= 0.6 | strong_cor <= -0.6, arr.ind = T)
strong_cor_var = cbind.data.frame(var1 = rownames(strong_cor)[index[,1]], # get the row name 
                 var2 = colnames(strong_cor)[index[,2]]) # get the column name
strong_cor_var
```
From the correlation graph above, we can clearly find that [BsmtFinSF1, BsmtFullBath], [TotalBsmtSF, X1stFlrSF], [GrLivArea, X2ndFlrSF], [GrLivArea, TotRmsAbvGrd] have appear to be potentially problematic collinearity amongst the predictor variables.


### EDA - (2) graph

```{r}
dim(train)
```

There are 1460 observations and 81 variables in the training data set.
First, lets's start to explore the response - Sale Price (in dollars).


```{r}
ggplot(train, aes(x = SalePrice)) + 
  geom_histogram(bins = 50, col= "white") 
```

The plot is right-skewed, which means that there is less expensive house than inexpensive ones.


```{r}
summary(train$SalePrice)
```

The median of Sale Price of the houses is $163000.
The mean of Sale Price of the houses is $180921.
The least expensive house is $34900.
The most expensive house is $755000.


Next, to investigate if there are early signs of variables are likely to be significant in predicting response. First, let's look at those numeric variables.

```{r}
corr_xy = cor(train[,unlist(lapply(train, is.numeric))])
y_col = ncol(cor(train[,unlist(lapply(train, is.numeric))]))
cor(train[,unlist(lapply(train, is.numeric))])[,y_col]
corr_xy_df = cbind.data.frame(SalePrice=cor(train[,unlist(lapply(train, is.numeric))])[,y_col])
corr_xy_df
```

From the last part, we find that the variables $TotalBsmtSF$, $X1stFlrSF$, $GrLivArea$, $GarageCars$, $GarageArea$ have strong correlation with the response $SalePrice$.

```{r}
strong_collearity <- subset(train, select = c(TotalBsmtSF, X1stFlrSF, GrLivArea, SalePrice))
pairs.panels(strong_collearity)
```

From the plot, we can verify that all of these three variables $TotalBsmtSF$ (Total square feet of basement area), $X1stFlrSF$ (First Floor square feet), $GrLivArea$ (Above grade (ground) living area square feet) have strong positive correlation with the response $SalePrice$. In other words, as each of these three factors (Total square feet of basement area / First Floor square feet / Above grade (ground) living area square feet) increasing, $SalePrice$ will get increase.

In addition, we would also explore those categorical variables that might be useful for predicting the response. 

$OverallQual$:

```{r}
overallQual <- as.numeric(train$OverallQual)
cor(overallQual, train$SalePrice)
```

Thus, $OverallQual$ has a strong positive correlation with the $SalePrice$.


```{r}
ggplot(train, aes(x=overallQual,y=SalePrice)) + geom_point(shape=20) + geom_smooth(method="lm", color = "red")+ scale_y_continuous(labels=comma)
```

This plot verifies that $OverallQual$ has a strong positive correlation with the $SalePrice$.


```{r}
train$OverallQual=as.factor(train$OverallQual)
train%>%ggplot(aes(x=OverallQual,y=SalePrice))+geom_violin(aes(fill=OverallQual))
```

OverallQual: Rates the overall material and finish of the house.

       10	Very Excellent
       9	Excellent
       8	Very Good
       7	Good
       6	Above Average
       5	Average
       4	Below Average
       3	Fair
       2	Poor
       1	Very Poor



From the colorful plot, we can find that as Rates the overall material and finish of the house increasing, the sale price of the house gets increased. Besides, if the rates the overall material and finish of the house is below average, the sale price varies for the largest range other than that of other rates. In addition, there is no big difference of the mean sale price of house at rate = 6 and rate = 7.


Also, we can find that $Neighboorhood$ is also a good predictor for its positive strong correlation with $SalePrice$.

```{r}
cor(train$TotalBsmtSF , train$SalePrice)
```

```{r}
ggplot(train, aes(x=Neighborhood,y=SalePrice)) + geom_boxplot(fill="light blue", color="black")+
theme(axis.text.x=element_text(angle = 90)) + scale_y_continuous(labels=comma)
                                            # change the scale from e-x type into real number
```

The neighborhoods of the house plays a significant role in the sale price. We can see that the houses around MeadowV were sold at the least expensive price, while those besides StroneBr were sold at the most expensive price. For houses' neighborhood is NoRidge, there are some of the most expensive price.


Moreover, for the $GarageCars$, we would like to explore by plotting.

```{r}
train$GarageCars=as.factor(train$GarageCars)
train%>%ggplot(aes(x=GarageCars,y=SalePrice))+geom_violin(aes(fill=GarageCars))
```

We can see that for the house of size of garage in car capacity as 3, the sale price is the highest, while the houses with size of garage in car capacity as 0 have the most inexpensive sale price.

At last, we infer that the age of house might be an important predictor.

House's age since being built (2022-YearBuilt):

```{r}
year <- as.numeric(train$YearBuilt)
AgeHouse <- 2022 - year
```

```{r}
cor(AgeHouse, train$SalePrice)
```

It shows that there is strong negative correlation between the age of house and the sale price.

```{r}
ggplot(train, aes(x=AgeHouse,y=SalePrice)) + geom_point(shape=20) + geom_smooth(method="lm", color = "red")+ scale_y_continuous(labels=comma)
```

As the age of house increases, the sale price will get decreasing.


In brief, based on the EDA, there are lots of variables having weak correlation with the response. Thus, we would like to choose the LASSO and ridge regression for modeling.


## Part II: Model Analysis
### Motivation 
Housing market is an important sector of the economy. Having an accurate prediction of housing price is of interest to the general public and the economic forecast. Conventional models for predictions include regression, decision trees, naive bayes, recurrent neural networks, etc. A good model should be able to generalize well to the test data, meaning that it should aim to capture the global minimum (in a strong convex optimization problem) without over-fitting or under-fitting, this means we need to take into account the bias-variance trade-off. Motivated by these insights, we propose using Lasso (L1 regularization) based logistic regression model for this particular task. Fitting a lasso-based regression model should ensure enough model capacity while minimizing the chance of over-fitting through regularization. It also has advantage over neural network given our limited amount of data samples. To find the best modeling strategy, we also test Ridge along with Lasso.

### Math
LASSO:
$$\min_{\beta _{0},\beta } \left\{\sum_{i=1}^{N}\left ( y_{i}-\beta _{0}-x_{i}^{T}\beta  \right )^{2}\right\} subject \; to \sum_{j=1}^{p}\left| \beta _{j}\right|\leq t$$
Ridge:
$$\min_{\beta _{0},\beta } \left\{\sum_{i=1}^{N}\left ( y_{i}-\beta _{0}-x_{i}^{T}\beta  \right )^{2}\right\} subject \; to \sum_{j=1}^{p} \beta _{j}^{2}\leq t$$
t is a prespecified free parameter that determines the degree of regularization.

### Assumption

### Validation
Here we explain one important step to the data augmentation. For fairness purposes, we need to ensure that our train and test distributions are the same (e.g. predictors existing in test data must also exist in train data). For this reason, we first compute the confusion matrix of both the train and test data, we then drop the features that are in the disjoint set of train and test sets. 

### Model & Result
We evaluate our models by mean squared error and our result shows that Lasso out-performs Ridge.
```{R}
x_train <-  model.matrix(SalePrice~ . -1 , data = train)
y_train <-  train$SalePrice
x_test <-  model.matrix(SalePrice~ . -1 , data = test)
y_test <-  test$SalePrice
print(dim(x_train))
print(dim(x_test))
#x_train-x_test
missing_cols = c()
for (var in colnames(x_train)){
  if (!(var %in% colnames(x_test))){
    missing_cols <- c(missing_cols, var)
  }
}

#x_test-x_train
missing_cols_2 = c()
for (var in colnames(x_test)){
  if (!(var %in% colnames(x_train))){
    missing_cols_2 <- c(missing_cols_2, var)
  }
}
#we simply remove the mismatch to ensure the same distribution between train and test dataset
x_train <- x_train[, !colnames(x_train) %in% missing_cols]
x_test <- x_test[, !colnames(x_test) %in% missing_cols_2]

train = train[, !colnames(train) %in% missing_cols]
train = train[, !colnames(train) %in% missing_cols_2]

test = test[, !colnames(test) %in% missing_cols]
test = train[, !colnames(train) %in% missing_cols]
```

#### lasso
```{R}
cv.lasso <-  cv.glmnet(x_train, y_train, type.measure = "mse", alpha = 1)
plot(cv.lasso)
model.lasso =glmnet(x_train,y_train,lambda=cv.lasso$lambda.min, alpha=1)
pred <- predict(model.lasso,x_test)
mse_lasso = mean((pred-y_test)^2)
mse_lasso
```

#### ridge
```{R}
cv.ridge <-  cv.glmnet(x_train, y_train, type.measure = "mse", alpha = 0)
plot(cv.ridge)
model.ridge = glmnet(x_train,y_train,lambda=cv.ridge$lambda.min, alpha=0)
pred <- predict(model.ridge,x_test)
mse_ridge = mean((pred-y_test)^2)
mse_ridge
```

#### pcr
```{R}
model.pcr = pcr(SalePrice~.,data=train, validation="CV")
# summary(model.pcr)
validationplot(model.pcr,val.type="MSEP", ylim=c(0,9999999999))
model.pcr2 = pcr(SalePrice~.,data=train,scale=TRUE,ncomp=170)
pcr.pred=predict(model.pcr2,ncomp=170)
mse_pcr = mean((as.vector(pcr.pred)-y_test)^2)
mse_pcr
```

#### plsr
```{R}
model.plsr = plsr(SalePrice~.,data=train,validation="CV")
# summary(model.plsr)
validationplot(model.plsr,val.type="MSEP")
model.plsr2 = plsr(SalePrice~.,data=train,scale=TRUE,ncomp=23)
pls.pred=predict(model.plsr2,ncomp=23)
mse_pls = mean((as.vector(pls.pred)-y_test)^2)
mse_pls
```
